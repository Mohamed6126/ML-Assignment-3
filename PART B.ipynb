{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda7b8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Adult dataset...\n",
      "Dataset shape: (48842, 14)\n",
      "Categorical columns found: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
      "\n",
      "=== Running: Full ===\n",
      "Features: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
      "Features used: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
      "alpha=0.1 val_acc=0.7891\n",
      "alpha=0.5 val_acc=0.7890\n",
      "alpha=1.0 val_acc=0.7889\n",
      "alpha=2.0 val_acc=0.7886\n",
      "alpha=5.0 val_acc=0.7890\n",
      "Selected best alpha: 0.1\n",
      "Custom NB test metrics: {'accuracy': 0.7951412583594923, 'precision_macro': 0.7307525938177326, 'recall_macro': 0.7748298012264996, 'f1_macro': 0.745116303582243}\n",
      "Sklearn MultinomialNB test metrics: {'accuracy': 0.760747918657022, 'precision_macro': 0.380373959328511, 'recall_macro': 0.5, 'f1_macro': 0.43205953026897137}\n",
      "\n",
      "=== Running: Top3 ===\n",
      "Features: ['workclass', 'education', 'occupation']\n",
      "Features used: ['workclass', 'education', 'occupation']\n",
      "alpha=0.1 val_acc=0.7691\n",
      "alpha=0.5 val_acc=0.7691\n",
      "alpha=1.0 val_acc=0.7691\n",
      "alpha=2.0 val_acc=0.7689\n",
      "alpha=5.0 val_acc=0.7683\n",
      "Selected best alpha: 0.1\n",
      "Custom NB test metrics: {'accuracy': 0.7797188480960829, 'precision_macro': 0.6916810452631404, 'recall_macro': 0.6612617132227678, 'f1_macro': 0.6726015690613805}\n",
      "Sklearn MultinomialNB test metrics: {'accuracy': 0.760747918657022, 'precision_macro': 0.380373959328511, 'recall_macro': 0.5, 'f1_macro': 0.43205953026897137}\n",
      "\n",
      "=== Running: Top6 ===\n",
      "Features: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'sex']\n",
      "Features used: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'sex']\n",
      "alpha=0.1 val_acc=0.7905\n",
      "alpha=0.5 val_acc=0.7905\n",
      "alpha=1.0 val_acc=0.7905\n",
      "alpha=2.0 val_acc=0.7901\n",
      "alpha=5.0 val_acc=0.7898\n",
      "Selected best alpha: 0.1\n",
      "Custom NB test metrics: {'accuracy': 0.7948682953459806, 'precision_macro': 0.7306721876775456, 'recall_macro': 0.77523696626686, 'f1_macro': 0.7450831400765064}\n",
      "Sklearn MultinomialNB test metrics: {'accuracy': 0.760747918657022, 'precision_macro': 0.380373959328511, 'recall_macro': 0.5, 'f1_macro': 0.43205953026897137}\n",
      "\n",
      "=== Running: Full_minus_occupation ===\n",
      "Features: ['workclass', 'education', 'marital-status', 'relationship', 'race', 'sex', 'native-country']\n",
      "Features used: ['workclass', 'education', 'marital-status', 'relationship', 'race', 'sex', 'native-country']\n",
      "alpha=0.1 val_acc=0.7648\n",
      "alpha=0.5 val_acc=0.7647\n",
      "alpha=1.0 val_acc=0.7646\n",
      "alpha=2.0 val_acc=0.7646\n",
      "alpha=5.0 val_acc=0.7640\n",
      "Selected best alpha: 0.1\n",
      "Custom NB test metrics: {'accuracy': 0.7716664391974888, 'precision_macro': 0.7132780724787557, 'recall_macro': 0.7674174734746586, 'f1_macro': 0.7259716988840308}\n",
      "Sklearn MultinomialNB test metrics: {'accuracy': 0.760747918657022, 'precision_macro': 0.380373959328511, 'recall_macro': 0.5, 'f1_macro': 0.43205953026897137}\n",
      "\n",
      "=== Summary Table ===\n",
      "              experiment                                           features  \\\n",
      "0                   Full  workclass,education,marital-status,occupation,...   \n",
      "1                   Top3                     workclass,education,occupation   \n",
      "2                   Top6  workclass,education,marital-status,occupation,...   \n",
      "3  Full_minus_occupation  workclass,education,marital-status,relationshi...   \n",
      "\n",
      "   best_alpha  custom_acc  custom_prec  custom_rec  custom_f1  sklearn_acc  \\\n",
      "0         0.1    0.795141     0.730753    0.774830   0.745116     0.760748   \n",
      "1         0.1    0.779719     0.691681    0.661262   0.672602     0.760748   \n",
      "2         0.1    0.794868     0.730672    0.775237   0.745083     0.760748   \n",
      "3         0.1    0.771666     0.713278    0.767417   0.725972     0.760748   \n",
      "\n",
      "   sklearn_f1  \n",
      "0     0.43206  \n",
      "1     0.43206  \n",
      "2     0.43206  \n",
      "3     0.43206  \n",
      "\n",
      "Finished running all experiments.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "#  Load dataset\n",
    "def load_adult():\n",
    "    df = fetch_openml('adult', version=2, as_frame=True)['frame']\n",
    "    target_col = 'class'\n",
    "    X = df.drop(columns=[target_col]).copy()\n",
    "    y_raw = df[target_col].astype(str).str.strip()\n",
    "    y = y_raw.replace({'<=50K.': '<=50K', '<=50K': '<=50K', '>50K.': '>50K', '>50K': '>50K'})\n",
    "    y = (y == '>50K').astype(int)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_categorical_columns(X):\n",
    "    return [c for c in X.columns if X[c].dtype == 'object' or X[c].dtype.name == 'category']\n",
    "\n",
    "\n",
    "# Preprocess categorical: replace missing and ordinal-encode\n",
    "\n",
    "def preprocess_categorical(X, selected_features=None):\n",
    "    Xc = X.copy()\n",
    "    if selected_features is None:\n",
    "        cat_cols = get_categorical_columns(Xc)\n",
    "    else:\n",
    "        cat_cols = selected_features\n",
    "    for c in cat_cols:\n",
    "        Xc[c] = Xc[c].astype(str).str.strip().replace({'?': 'MISSING', '': 'MISSING', 'nan': 'MISSING', 'None': 'MISSING'})\n",
    "        Xc[c] = Xc[c].fillna('MISSING')\n",
    "    enc = OrdinalEncoder(dtype=int)\n",
    "    X_enc = enc.fit_transform(Xc[cat_cols])\n",
    "    n_values = [len(enc.categories_[i]) for i in range(len(cat_cols))]\n",
    "    return X_enc.astype(int), enc, cat_cols, n_values, Xc[cat_cols]\n",
    "\n",
    "\n",
    "# Categorical Naive Bayes manually\n",
    "\n",
    "class CategoricalNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.classes_ = None\n",
    "        self.class_priors_ = None\n",
    "        self.feature_cond_probs_ = None\n",
    "        self.n_values_per_feature = None\n",
    "\n",
    "    def fit(self, X, y, n_values_per_feature):\n",
    "        N, D = X.shape\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        K = len(classes)\n",
    "        self.classes_ = classes\n",
    "        self.n_values_per_feature = n_values_per_feature\n",
    "        alpha = self.alpha\n",
    "        self.class_priors_ = (counts + alpha) / (N + alpha * K)\n",
    "        self.feature_cond_probs_ = []\n",
    "        for j in range(D):\n",
    "            nv = n_values_per_feature[j]\n",
    "            counts_j = np.zeros((K, nv))\n",
    "            for ki, cls in enumerate(classes):\n",
    "                mask = (y == cls)\n",
    "                vals = X[mask, j]\n",
    "                for v in range(nv):\n",
    "                    counts_j[ki, v] = np.sum(vals == v)\n",
    "            denom = counts.reshape(-1, 1) + alpha * nv\n",
    "            probs_j = (counts_j + alpha) / denom\n",
    "            self.feature_cond_probs_.append(probs_j)\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        N, D = X.shape\n",
    "        K = len(self.classes_)\n",
    "        log_priors = np.log(self.class_priors_)\n",
    "        log_probs = np.zeros((N, K))\n",
    "        for i in range(N):\n",
    "            x = X[i]\n",
    "            for k in range(K):\n",
    "                s = log_priors[k]\n",
    "                for j in range(D):\n",
    "                    v = x[j]\n",
    "                    if v < self.feature_cond_probs_[j].shape[1]:\n",
    "                        s += np.log(self.feature_cond_probs_[j][k, v])\n",
    "                    else:\n",
    "                        s += np.log(1e-12)\n",
    "                log_probs[i, k] = s\n",
    "        return log_probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        idx = np.argmax(self.predict_log_proba(X), axis=1)\n",
    "        return self.classes_[idx]\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    }\n",
    "\n",
    "#  run_experiment: train/val/test split, hyperparameter search, final eval\n",
    "\n",
    "def run_experiment(X_df, y, feature_subset=None, alphas=[0.1,0.5,1.0,2.0,5.0], random_state=42):\n",
    "    X_enc, enc, cat_cols, n_values, Xc_original = preprocess_categorical(X_df, selected_features=feature_subset)\n",
    "    \n",
    "    print(\"Features used:\", cat_cols)\n",
    "        \n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X_enc, y, test_size=0.15, stratify=y, random_state=random_state)\n",
    "    val_ratio = 0.15 / 0.85\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_ratio, stratify=y_temp, random_state=random_state)\n",
    "   \n",
    "    results = []\n",
    "    for a in alphas:\n",
    "        model = CategoricalNaiveBayes(alpha=a)\n",
    "        model.fit(X_train, y_train, n_values)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        metrics = compute_metrics(y_val, y_val_pred)\n",
    "        metrics['alpha'] = a\n",
    "       \n",
    "        print(f\"alpha={a} val_acc={metrics['accuracy']:.4f}\")\n",
    "        results.append(metrics)\n",
    "\n",
    "    best = max(results, key=lambda r: r['accuracy'])\n",
    "    best_alpha = best['alpha']\n",
    "  \n",
    "    print(\"Selected best alpha:\", best_alpha)\n",
    "\n",
    "    X_comb = np.vstack([X_train, X_val])\n",
    "    y_comb = np.concatenate([y_train, y_val])\n",
    "    final_model = CategoricalNaiveBayes(alpha=best_alpha)\n",
    "    final_model.fit(X_comb, y_comb, n_values)\n",
    "    y_test_pred = final_model.predict(X_test)\n",
    "    final_metrics = compute_metrics(y_test, y_test_pred)\n",
    "    \n",
    "    print(\"Custom NB test metrics:\", final_metrics)\n",
    "\n",
    "    # Sklearn MultinomialNB comparison (one-hot)\n",
    "    X_all_str = Xc_original\n",
    "    X_train_str_ohe = X_all_str.iloc[:len(X_train)].values\n",
    "    X_val_str_ohe = X_all_str.iloc[len(X_train):len(X_train)+len(X_val)].values\n",
    "    X_test_str_ohe = X_all_str.iloc[len(X_train)+len(X_val):len(X_enc)].values\n",
    "    \n",
    "    X_comb_str_ohe = np.vstack([X_train_str_ohe, X_val_str_ohe])\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    X_ohe_comb = ohe.fit_transform(X_comb_str_ohe)\n",
    "    X_ohe_test = ohe.transform(X_test_str_ohe)\n",
    "\n",
    "    mnb = MultinomialNB(alpha=best_alpha)\n",
    "    mnb.fit(X_ohe_comb, y_comb)\n",
    "    y_test_pred_mnb = mnb.predict(X_ohe_test)\n",
    "    sklearn_metrics = compute_metrics(y_test, y_test_pred_mnb)\n",
    "   \n",
    "    print(\"Sklearn MultinomialNB test metrics:\", sklearn_metrics)\n",
    "\n",
    "    return {\n",
    "        'validation_results': results,\n",
    "        'best_alpha': best_alpha,\n",
    "        'test_custom': final_metrics,\n",
    "        'test_sklearn': sklearn_metrics,\n",
    "        'feature_names': cat_cols,\n",
    "        'n_values': n_values,\n",
    "        'Xc_original': Xc_original\n",
    "    }\n",
    "\n",
    "\n",
    "#  run a list of manual subsets \n",
    "\n",
    "def run_manual_subsets(X_df, y, manual_subsets):\n",
    "    rows = []\n",
    "    for name, feat_list in manual_subsets:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\\n=== Running:\", name, \"===\\nFeatures:\", feat_list)\n",
    "        res = run_experiment(X_df, y, feature_subset=feat_list)\n",
    "        test_custom = res['test_custom']\n",
    "        test_sklearn = res['test_sklearn']\n",
    "        rows.append({\n",
    "            'experiment': name,\n",
    "            'features': ','.join(feat_list),\n",
    "            'best_alpha': res['best_alpha'],\n",
    "            'custom_acc': test_custom['accuracy'],\n",
    "            'custom_prec': test_custom['precision_macro'],\n",
    "            'custom_rec': test_custom['recall_macro'],\n",
    "            'custom_f1': test_custom['f1_macro'],\n",
    "            'sklearn_acc': test_sklearn['accuracy'],\n",
    "            'sklearn_f1': test_sklearn['f1_macro']\n",
    "        })\n",
    "    df_results = pd.DataFrame(rows)\n",
    "    print(\"\\n=== Summary Table ===\")\n",
    "    print(df_results)\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading Adult dataset...\")\n",
    "    X_df, y = load_adult()\n",
    "    print(\"Dataset shape:\", X_df.shape)\n",
    "    cat_cols = get_categorical_columns(X_df)\n",
    "    print(\"Categorical columns found:\", cat_cols)\n",
    "\n",
    "    subset_full = cat_cols.copy()\n",
    "    subset_top3 = [c for c in cat_cols if c in ['education','occupation','workclass']][:3]\n",
    "    subset_top6 = [c for c in cat_cols if c in ['education','occupation','workclass','marital-status','relationship','sex']]\n",
    "    subset_minus_occupation = [c for c in subset_full if c != 'occupation']\n",
    "\n",
    "    manual_subsets = [\n",
    "        ('Full', subset_full),\n",
    "        ('Top3', subset_top3),\n",
    "        ('Top6', subset_top6),\n",
    "        ('Full_minus_occupation', subset_minus_occupation)\n",
    "    ]\n",
    "\n",
    "    results_df = run_manual_subsets(X_df, y, manual_subsets)\n",
    "\n",
    "    print(\"\\nFinished running all experiments.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
